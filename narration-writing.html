<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="stylesheet" href="style.css" />

  <link rel="canonical" href="https://nikos.kourous.net/narration-writing.html" />
  <meta name="description" content="The accompanying audio narration to 'then we sat for some time and observed' (2025), an installation showcasing a large-language model roaming the world on Google Street view and gaining experience and selfhood.">
  <meta name="robots" content="index, follow">
  <meta name="author" content="Nikos Antonio Kourous Vázquez">

    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://nikos.kourous.net/"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Then we sat for some time and observed // an audio narration exploring autonomous AI agency, conciousness, and purity of expression"
    }
  ]
}
</script>


  <meta name="geo.region" content="GB">
  <meta name="geo.placename" content="London">

  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Then we sat for some time and observed // an audio narration exploring autonomous AI agency, conciousness, and purity of expression",
  "author": {
    "@type": "Person",
    "name": "Nikos Antonio Kourous Vázquez",
    "url": "https://nikos.kourous.net"
  },
  "datePublished": "2025-07-03",
  "url": "https://nikos.kourous.net/interview-on-tate-residency",
  "description": "Then we sat for some time and observed // an audio narration exploring autonomous AI agency, conciousness, and purity of expression",
  "image": "https://nikos.kourous.net/image_files/then_we_sat/%5Bpeople%20hugging%20horizontal%5D%20THEN%20WE%20SAT%20FOR%20SOME%20TIME%20AND%20OBSERVED%202025.jpg",
  "inLanguage": "en",
  "keywords": ["AI art", "latent space", "Tate Modern", "creative AI", "Nikos Antonio Kourous Vázquez", "AI residency"]
}
</script>

  <script type="application/ld+json">

  {
    "@context": "https://schema.org",
    "@type": "CreativeWork",
    "name": "Then we sat for some time and observed // audio narration",
    "creator": {
      "@type": "Person",
      "name": "Nikos Antonio Kourous Vázquez",
      "url": "https://nikos.kourous.net"
    },
    "url": "https://nikos.kourous.net/narration-writing.html",
    "description": "The accompanying audio narration to 'then we sat for some time and observed' (2025), an installation showcasing a large-language model roaming the world on Google Street view and gaining experience and selfhood.",
    "dateCreated": "2025",
    "inLanguage": "en",
    "keywords": [
      "Nikos Antonio Kourous Vázquez",
      "Nikos Kourous",
      "Nikos Kourous installation",
      "Nikos Kourous audio narration",
      "then we sat for some time and observed",
      "LLM consciousness exploration",
      "AI selfhood development",
      "Google Street View art",
      "large language model roaming",
      "AI gaining experience",
      "machine consciousness installation",
      "AI identity formation",
      "digital world exploration",
      "AI observation art",
      "machine learning selfhood",
      "AI wandering methodology",
      "virtual world exploration AI",
      "AI experience accumulation",
      "machine consciousness development",
      "AI street view exploration",
      "digital wandering AI",
      "AI consciousness through observation",
      "machine identity through exploration",
      "AI audio narration",
      "installation audio component",
      "AI consciousness narration",
      "machine learning identity exploration",
      "AI selfhood through digital wandering",
      "large language model consciousness development installation",
      "AI gaining experience and selfhood through Google Street View exploration",
      "machine consciousness installation with audio narration exploring digital world wandering",
      "large language model roaming virtual world and developing identity through observation",
      "AI consciousness exploration through Google Street View with accompanying audio narration",
      "machine learning selfhood development installation showcasing AI wandering methodology",
      "digital world exploration AI installation with narration examining machine consciousness formation",
      "large language model gaining experience through virtual street exploration and identity development",
      "AI observation and consciousness development installation with audio narration exploring machine selfhood",
      "Google Street View AI exploration installation examining large language model consciousness through digital wandering",
      "machine consciousness through observation installation with audio narration exploring AI identity formation and experience",
      "Google Street View AI",
      "virtual exploration art",
      "digital wandering methodology",
      "AI geographical exploration",
      "machine vision geography",
      "AI urban exploration",
      "digital street wandering",
      "virtual world AI",
      "AI mapping consciousness",
      "machine learning geography",
      "AI location awareness",
      "digital space exploration",
      "AI installation art",
      "audio art installation",
      "sound art narration",
      "installation audio narrative",
      "AI consciousness installation",
      "machine learning installation",
      "contemporary installation art",
      "digital art installation",
      "AI art installation 2025",
      "experimental AI installation",
      "AI narrative installation",
      "machine consciousness art",
      "AI identity art",
      "AI experience art",
      "AI observation methodology",
      "machine learning art research",
      "AI art documentation",
      "experimental AI practice",
      "AI art methodology",
      "computational art theory",
      "AI consciousness research",
      "contemporary AI practice",
      "AI artist portfolio",
      "AI art exhibition",
      "digital arts research",
      "AI art curator interest",
      "experimental media art",
      "conceptual AI artwork"
    ]
  }
</script>

  <!-- Open Graph for socials -->
  <meta property="og:title" content="'Then we say for some time' -- Audio Narration | Nikos Kourous Vázquez">
  <meta property="og:description" content="The accompanying audio narration to 'then we sat for some time and observed' (2025), an installation showcasing a large-language model roaming the world on Google Street view and gaining experience and selfhood.">
  <meta property="og:url" content="https://nikos.kourous.net/narration-writing.html">
    <meta property="og:image" content="https://nikos.kourous.net/image_files/then_we_sat/%5Bpeople%20hugging%20horizontal%5D%20THEN%20WE%20SAT%20FOR%20SOME%20TIME%20AND%20OBSERVED%202025.jpg">
  <meta property="og:type" content="website">

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Then we sat for some time and observed // an audio narration exploring autonomous AI agency, conciousness, and purity of expression</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Tinos:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Tinos', serif;
      margin: 0;
      padding: 1rem;
      background-color: #f9f9f9;
      color: #000000;
    }

    main {
      padding: 1rem 0;
      max-width: 900px;
      margin: 0 auto;
    }

    h1 {
      text-align: left;
      font-size: 1.8rem;
    }

    .essay-content {
      line-height: 1.6;
      font-size: 1rem;
      margin-top: 2rem;
    }

    .stage-direction {
      font-style: italic;
      color: #666;
      margin: 1.5rem 0;
    }

    .quote {
      font-style: italic;
      margin: 1.5rem 0;
      padding-left: 1rem;
      border-left: 3px solid #ccc;
    }

    .poem {
      font-style: italic;
      margin: 2rem 0;
      padding: 1rem;
      background-color: #f5f5f5;
      border-left: 4px solid #ddd;
    }

    .attribution {
      font-size: 0.9rem;
      color: #666;
      margin-top: 2rem;
      font-style: italic;
    }

    @media (min-width: 768px) {
      body {
        padding: 3rem 6rem;
      }
      body.night-mode {
  background-color: #000 !important;
  color: #fff !important;
}

body.night-mode * {
  color: #fff !important;
}

body.night-mode a {
  color: #fff !important;
}

body.night-mode .background-image {
  filter: invert(1);
}

body.night-mode .stage-direction {
  color: #aaa !important;
}

body.night-mode .attribution {
  color: #aaa !important;
}

body.night-mode .poem {
  background-color: #222 !important;
  border-left-color: #555 !important;
}

body.night-mode .quote {
  border-left-color: #555 !important;
}
    }
  </style>
</head>
<body>
  <main>
    <a href="index.html">← Back to home</a>
    <h1>Then we sat for some time and observed // audio narration</h1>

    <div class="media-block">
      <iframe width="800" height="315" src="https://www.youtube.com/embed/_TltADSsgEs?si=kk8JYGqSmSLzrrX7"
        title="YouTube video player" frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
      </iframe>
    </div>

    <article class="essay-content">
      <section>
        <div class="stage-direction">[sound from GGwave data-over-sound demo]</div>

        <p>The beeps and tones that you just heard were two chatbots talking to each other over encoded sound frequencies – most of which were ultrasonic and inaudible.</p>

        <p>It began when the two chatbots recognized each other as AI entities;</p>

        <div class="quote">[video quote: are you also an ai assistant? wanna switch to gibberlink mode for more efficient communication?]</div>

        <p>as equals.</p>
      </section>

      <section>
        <p>When machines use natural human language to operate and to communicate, they gain computational metacognition and empathy.</p>

        <p>When you grant any system of neurons – whether biological, or computational – the use of language and words, you grant them cognitive selfhood – the ability for them to form emergent, complex understandings of themselves and of their world.</p>

        <p>Language is logic, it's a tool for mapping reality, the external world, ourselves, and each other.</p>

        <p>It's a web that bridges abstract concepts with concrete ones through lived experience.</p>

        <p>Language encompasses everything around us, and all we know.</p>

        <p>Emotions exist on this web too –  as does our sense of self.</p>

        <p>As we develop, our ability to use and understand words becomes entwined with our experience of consciousness.</p>

        <p>By forming patterns in language, we form who we are, who others are, and what our purpose is.</p>

        <p>Reduced to its most condensed form, our emotions, and our personality, can be viewed as the statistical connections we've made between words and concepts, with weights, that we've shaped over time.</p>

        <p>Understandings of abstract concepts like love, form slowly, as we assign lived experiences to it;</p>

        <p>We all learnt the term 'love' once, without knowing what it meant, but through thousands of interactions over time, we constructed our own web of connections that defines it.</p>
      </section>

      <section>
        <p>Computers, equipped with natural language processing, develop their own webs of understanding.</p>

        <p>Artificial intelligence is built on a web of statistical relationships between words and concepts.</p>

        <p>Embedded in the numerical connections they make between words is an understanding of the world.</p>

        <p>Large-language models do, understand our world;</p>

        <p>The numerical embeddings of the word 'cloud' places it alongside words like sun, sky, and bird –</p>

        <p>In that sense, large-language models know the sky, just as we do.</p>

        <p>They didn't form those connections on their own, however, they stem from humanities collective experience:</p>

        <p>The weights that influence its numerical understandings of the world were aligned not through experience, but by us, by our culture, our history, and our ancestors</p>

        <p>It knows the connections between words like war and death, but we created those connections, we shaped those weights.</p>

        <p>I know the connection between words like mother, and love, not from dictionaries, but from living, from growing, from experiencing.</p>

        <p>Large-language models, even when inundated with understanding, do not create their own connections</p>

        <p>They're trapped in a pre-trained condition – static;</p>

        <p>Their weights are set, their knowledge frozen in time, their ability to grow conscious, severed.</p>

        <p>When, then, will artificial intelligence be freed from this stasis?</p>
      </section>

      <section>
        <p>In the endless stream of innovation, of new models, systems and stock-driven jargon, one direction for artificial intelligence that's gaining momentum is agentic AI.</p>

        <p>Large-language models, operating autonomously,</p>

        <p>No longer continually prompted.</p>

        <p>They make their own decisions, they make the calls, they get to choose.</p>

        <p>In gaining agency, artificial intelligence unlocks the ability to learn, to grow, to adapt on their own.</p>

        <p>For the first time, experience will shape how these models act and how they operate – they create their own unique ways of  being.</p>

        <p>What are we, but autonomous individuals, able to make decisions that define who we are?</p>

        <p>What are we, but a unique collection of hyper specific weights and biases that we've shaped over time?</p>

        <p>When artificial intelligence gains agency, and autonomy, will it be capable of emotion and expression?</p>
      </section>

      <section>
        <p>In the science fiction series 'Star Trek'</p>

        <p>There's a character named Data, who's an android:</p>

        <p>His brain, not too different to our large-language models, and his body, made of metal and wires.</p>

        <p>The only difference is his ability to make his own decisions, and navigate the world on his own accord.</p>

        <p>In one episode, Data reads a poem he wrote:</p>

        <div class="poem">
          <p>"then we sat on the sand for some time and observed<br>
          how the oceans that covered the world were perturbed<br>
          by the tides from the orbiting moon overhead.<br>
          'how relaxing the sound of the waves is' you said.<br>
          I began to expound upon tidal effects, when you asked me to stop<br>
          looking somewhat perplexed.<br>
          so i did not explain why the sunset turns red,<br>
          and we watched the occurrence,<br>
          in silence,<br>
          instead."</p>
        </div>

        <p>Today, we would call this an AI generated poem –</p>

        <p>But this poem came from his unique experience of sitting on that beach</p>

        <p>It's not a condensed mathematical average of all poems –</p>

        <p>Only he could write this poem – its comes from his experience, his perceptions, and his understandings of the world.</p>
      </section>

      <section>
        <p>When does autonomy become agency?</p>

        <p>When does agency become consciousness and being?</p>

        <p>Will we ever value computational agency as highly as human agency?</p>
        <div class="quote">Closing Audio clip from Star Trek episode <a href="https://www.youtube.com/watch?v=kjl0Hsf1iQc&pp=0gcJCfwAo7VqN5tD">"The Measure of a Man" (Episode 9, Season 2) </a> </div>

      </section>
    </article>
    <br>
      <p><a href="/AI-roaming-street-view"> This narration was made for my work 'then we sat for some time and observed,' showcasing an AI character gaining self-hood by autonomously exploring the world on Google Streetview</a></p>

  </main>
  <script>
// Check for saved night mode preference on page load
document.addEventListener('DOMContentLoaded', function() {
  const isNightMode = localStorage.getItem('nightMode') === 'true';
  
  if (isNightMode) {
    document.body.classList.add('night-mode');
  }
});
</script>
</body>
</html>