<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="author" content="Nikos Antonio Kourous Vázquez">
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Interview on Tate Residency</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Tinos:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Tinos', serif;
      margin: 0;
      padding: 1rem;
      background-color: #f9f9f9;
      color: #000000;
    }

    main {
      padding: 1rem 0;
      max-width: 900px;
      margin: 0 auto;
    }

    h1 {
      text-align: left;
      font-size: 1.8rem;
    }

    .essay-content {
      white-space: pre-wrap;
      line-height: 1.6;
      font-size: 1rem;
      margin-top: 2rem;
    }

    @media (min-width: 768px) {
      body {
        padding: 3rem 6rem;
      }
      body.night-mode {
  background-color: #000 !important;
  color: #fff !important;
}

body.night-mode * {
  color: #fff !important;
}

body.night-mode a {
  color: #fff !important;
}

body.night-mode .background-image {
  filter: invert(1);
}
    }
  </style>
</head>
<body>
  <main>
    <a href="index.html">&larr; Back to home</a>
    <h1>Interview on Tate Residency</h1>
    <div class="essay-content">
<b>1. The Tech, Tea + Exchange residency has seen a convergence of institutions; Tate, Anthropic, Goldsmiths, and UAL, each have their own motivations. How do you see these relationships as a para-institution? As a resident, what has been your experience of this?</b>

All the institutions coming together for this residency created an interesting dynamic as they each approached creative AI from different perspectives and with different goals. While UAL and Goldsmiths were more hands-off, Tate and Anthropic had more distinct goals. Anthropic came from a corporate angle; while their employees might admire creative work that comes from using their service, they’re fundamentally more interested in public narrative and corporate alignment than the work itself. At times, Anthropic’s goals seemed to clash with those of the artists, like they minimized details over the environmental impact of their technology, for example. Tate's approach seemed more cautious and keen to contextualize our exploration of AI within their brand image, encouraging residents to explore their collection while ensuring we label our work as ‘AI generated.’

We’re currently at a point in time where institutions – especially creative ones – are still defining their relationships with artificial intelligence. This process of discovery creates its own para-institution, muddled by opposing goals between the technology companies behind artificial intelligence, the artists using the technology, and the institution deploying its outcomes.


<b>2. Data is at the heart of AI, as its primary resource and one of its largest controversies. What are your practices of data acquisition for your work? Do you think artists should adopt the same standards as corporations?</b>

AI company data acquisition mal-practice, on top of data already stifled by binary, aged, and western perspectives, inherently leaves a stain on creative work with AI. However, while training your own models or acquiring your own data is definitely a strong and smart approach as an artist, it also leaves you unable to critically engage with AI’s existing stains. During this residency I used Anthropic’s pre-trained Claude LLM to create an autonomous agent that endlessly hallucinates things about itself. In doing so I let it speak for itself, and rather than hiding Claude’s blemishes, let them reveal themselves. In line with that, I think that generative media always needs to be contextualized within a specific conceptual or practical framework so as to not showcase it as the final work. While I definitely don’t think that artists should adopt the same standards as corporations, I also think that it's important to use existing models and engage with them critically and head-on.

That being said, I do believe institutions working with AI, including the AI companies themselves, should be obliged to be transparent with the source of their training data and their data acquisition methods.


<b>3. What are the computational aesthetics of a LLM chatbot system? How do they change your approach to learning and knowledge procurement? And what creative affordances do they provide?</b>

I think that there’s two approaches to the aesthetics of large-language models; LLMs as chatbots, and LLMs as vast architectures of language and meaning. The aesthetics of LLMs as chatbots concerns itself more about it as a sociotechnical phenomenon with tangible impact. Here, the focus would be more on chatbots themselves, their impact, their role, or their use. The aesthetics of LLMs as architectures of language focuses more on them as epistemological systems based on a statistical understanding of language and knowledge on a level beyond what was ever possible.

LLMs as products, such as Claude, are trained not to discourage your ambitions, or say ‘no,’ or ‘I don’t really know.’ As products, they’re designed to please their user. Because of this, I stay clear from asking chatbots for their opinions on my ideas or writing or work. They can help you achieve the steps needed to execute your ideas, but it’s important to cultivate your own intuition and self-confidence in your own ideas.


<b>4. When working with AI systems we are often playing with the 'Latent Space' of the model. What does that mean to you? What ways have you found of exploring it?</b>

I’d describe the latent space of AI models as the parts in their digital makeup that can’t be assigned to any one specific facet of their training data; emerging either from overlap in its data, or from a lack of data. The kind that emerges from overlap in data tends to reveal embedded biases, perspectives, and ways of thinking within models. Recently I’ve been exploring this more explicit version of latent space by observing how models behave differently under different given names. Through this line of experimentation, which allows large-language models to simulate lived experience, I’ve observed how different starting names alter their simulated experiences based on the cultural origin of their name. This approach plays with the model's latent space by revealing biases and tendencies that would otherwise be hidden.

The second form of latent space emerges from gaps in training data. Especially in the early days of image and generation, this latent space was often explored when training data was limited and models struggled to depict things accurately. The result would be strange machine hallucinations that depict almost-recognizable yet completely alien imagery. Earlier in 2021 and 2022 I explored this imagery through abstract animated short-films. This form of latent space is becoming more difficult to find as training data becomes increasingly larger. It’s also difficult to explore this kind of latent space in LLMs, since they’re very good at presenting hallucinations as reality. In my project at the Tate Modern, I created a character that evolved through hallucination. By giving a LLM no context about itself other than its name is ‘Reginald Gilbert the Third,’ I created a system that turned its hallucinations about itself into real character traits, allowing him to evolve and become more nuanced over time.


<b>5. What aesthetic aspect of AI most attracts or repels you?</b>

I used to be attracted to the weirdness and abstractness of generative AI outputs, but I’ve recently moved away from that as it’s become both more polished overall, but also more culturally accepted and familiar. Now, I’m interested in emergent behavior that arises when you leave AI systems to run in feedback systems on their own; in observing strangeness arise from a polished product. I am strongly repelled from any generative outcome, whether visual or text, that is not distorted or influenced in any way. In that sense, I’m now more interested in AI as a system that will soon have autonomy and agency rather than as a tool able to generate media.


<b>6. Do you think of AI as a radical flattening? Is a 'sameness' something you have encountered in your work or viewing outputs from these tools? Do you respond to this through your practice?</b>

I definitely do think that generated media, whether text or image or video, is extremely flat and boring. Because of this, I find it crucial to shift the focus of the work onto the process of generation rather than the generated output itself.

In my most recent work, which emerged from experiments during the Tate residency, I had a large-language model exploring the world through Google Street View, forming memories, experience, emotions, and a personality over time. The goal of this was to find the line between generated poetry and real poetry, and whether generated poetry could gain authentic expression.

This line of investigation was inspired by poetry written by Data, the android from Star Trek. Although his brain functions similarly to a large-language model, Data possesses agency and a distinct lived experience. When he writes poetry, he may be drawing on a vast archive of previously read works, but he writes from his own perspective, rooted in his unique existence.

I think that the ‘sameness’ of generative text comes from the current staticness of large-language models. I’m interested in the idea that this inherent ‘sameness’ will theoretically disappear once these systems achieve autonomy, independence, and experiential learning capabilities.
    </div>
  </main>
  <script>
// Check for saved night mode preference on page load
document.addEventListener('DOMContentLoaded', function() {
  const isNightMode = localStorage.getItem('nightMode') === 'true';
  
  if (isNightMode) {
    document.body.classList.add('night-mode');
  }
});
</script>
</body>

</html>
